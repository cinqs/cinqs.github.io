---
layout: post
title: 模型训练过程中数据的拆分使用
tags: [blog, ml]
---

> 这是一篇澄清式的文章，对入门数据挖掘或者是机器学习领域的同学有一些帮助。
> 可参阅 周志华 [《机器学习》](https://pan.baidu.com/s/1WH2omt6yGlOUXk9lQmTOUw) 一书的 2.3 章节

最近在和一些做机器学习 / 预测性分析的朋友聊天的时候，总是会对一些模型评估方面的名词，以及他们所代表的对象产生分歧。
原因大概就是来源于大家各自学习的渠道的不同造成的。于是最近找了一些资料，教科书阅读。总结了一下写在下面。

## k-fold cross-validation

> 其实对于这个也是有很多不同的理解的，我参阅的Stanford的一个教授的[讲义](http://statweb.stanford.edu/~tibs/sta306bfiles/cvwrong.pdf)

k-folder CV主要常见于 模型的算法选取，或者是见于算法的超参数选取的过程中。

- 将数据等分为5或者10份（shuffle firstly），可以选择其中的1份为验证集，选取另外的k-1份为训练集
- 计算在同一个模型 / 超参数组 下，用k-1份训练集训练出的k-1个模型，将这个模型作用于验证集（validation set）上得到 k-1 个残差 $$E_(\lambda)$$，可以使用

 $$CV(\lambda) = \frac{1}{K}\sum_{k=1}^K{E_k(\lambda)}$$
 
 来计算这个模型 / 超参组的综合残差
 
 > $$\lambda$$是某一个算法或者是某一个算法的一个超参数组
 
- 对于各个算法或者是超参组求取各自的综合残差，用这些来比较各个算法 / 超参组的相对优劣

NOTE: 使用这种办法评估模型的相对优劣时，缺乏对线上环境的模拟。因为线上环境的数据属于模型开发使用数据的**时间外样本**，时间外样本带来的分布bias可能是不可避免的，对于要求模型泛化能力的场景，需要在训练数据集，验证数据集之外添加 **时间外样本集**

## 训练，验证和测试样本集

> 参考 周教授《机器学习》一书的 *2.2 评估办法* 一章节

NOTE: 这是我们在实际开发过程中常用的办法

1. 首先应该将一个样本集按照时间点分为前后两段，后一段属于**测试样本集（test dataset）**。这一部分的大小在大数据的情况下不应该按照比例分，而应该按照时间跨度分。比如你的数据集共有12个月的跨度，你应该考虑使用最后两个月的跨度的数据集当做测试样本集

2. 时间切分点的前一部分的数据集是 **训练样本集**和**验证样本集**的集合。在小数据集的情况下可以使用比例（例如6:4）随机拆分这部分数据集为上述两部分数据集。但是在大数据集的情况下，保持这样一个比例会造成验证样本集过大导致额外的不必要的时间开支。应当选择一个足以覆盖这部分样本集分布的一部分再抽样。这部分数据集可以是20，000这样子，但是没必要100,000这样子的验证样本集

**NOTE 使用原则**： 你应当使用 训练样本集 训练模型，使用验证样本集来验证模型训练效果，使用测试样本集来 1. 测试模型的泛化效果 2. 评估模型的上线效果
